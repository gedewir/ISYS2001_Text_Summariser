{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_sumarisier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gedewir/ISYS2001_Text_Summariser/blob/main/text_sumarisier-binder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarisation\n",
        "\n",
        "Recall an earlier notebook where you were given a task to summarise online content and produce a report. We converted an audio file to text. Similarly, we could write a notebook to convert a PDF or Word document to text or Web pages to text. The strategy is to convert everything to text, summarise the text, and use the summary in the final report.\n",
        "\n",
        "# The Challenge\n",
        "\n",
        "Create a project to summarise text and publish the project in Binder. \n",
        "\n",
        "You can choose how to input the text.  Some ideas include pasting it into a string, reading from a file, extract from a PDF or a webpage. \n",
        " \n",
        "It is okay to follow a online tutoirial or youtube video but make sure you have some understanding of what you are doing. You can ask you tutor for help if needed.  They will either help search, or perhaps explain the code in a tutorial.\n"
      ],
      "metadata": {
        "id": "SmVEXRTA1wZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing all modules:\n",
        "!pip install -q requests\n",
        "!pip install -q spacy\n",
        "!pip install -q PyInputPlus\n",
        "!pip install -q bs4"
      ],
      "metadata": {
        "id": "gFWMCGhRT76m",
        "outputId": "6ad1e2e6-793a-4442-fb14-41c559735e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyInputPlus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysimplevalidate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stdiomask (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all modules\n",
        "import bs4 as bs\n",
        "import requests \n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "import pyinputplus as pyip\n",
        "\n",
        "# ask the user to input a URL\n",
        "user_url = pyip.inputURL('Enter a URL: ')\n",
        "\n",
        "# function to make url request and only output the body text\n",
        "def url_request(requested_url):\n",
        "  url_response = requests.get(requested_url)\n",
        "  url_body_text = url_response.text\n",
        "  return url_body_text\n",
        "\n",
        "# use bs4 to parse html of the requested URL\n",
        "def soupify_html(html_text):\n",
        "  soup = bs.BeautifulSoup(html_text, 'html.parser')\n",
        "  body_text = soup.find_all('p')\n",
        "  text = ''\n",
        "  for p in body_text:\n",
        "    text = text + p.get_text()\n",
        "  return text\n",
        "\n",
        "# state that we are using the English tokenzier\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function to process the text, extract the sentences and join it altogether using spacy nlp\n",
        "def summarised_text(text):\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # filter out stop words in text\n",
        "  stopwords = list(STOP_WORDS)\n",
        "  words = [token.text for token in doc if token.text not in stopwords and token.text not in punctuation]\n",
        "\n",
        "  # add repeating words together\n",
        "  word_frequency = {}\n",
        "  for word in text:\n",
        "    if word not in word_frequency:\n",
        "      word_frequency[word] = 1\n",
        "    else:\n",
        "      word_frequency[word] += 1 \n",
        "  # rank words on word frequency\n",
        "    sentence_scores = {}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequency.keys():\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequency[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequency[word.text.lower()]\n",
        "\n",
        "    # get the top 3 sentences with the highest scores\n",
        "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
        "\n",
        "    # concatenate the top sentences to generate the summary\n",
        "    summary = \"\"\n",
        "    for sentence in top_sentences:\n",
        "        summary += sentence.text.strip() + \" \"\n",
        "\n",
        "    return summary\n",
        "\n",
        "user_body_text = url_request(user_url)\n",
        "user_soup_text = soupify_html(user_body_text)\n",
        "user_url_summarised = summarised_text(user_soup_text)\n",
        "print('Summary:', user_url_summarised)"
      ],
      "metadata": {
        "id": "s9iCuO8Hekgp",
        "outputId": "f5e261ad-f4fe-4860-fa28-8c19e1956382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a URL: https://en.wikipedia.org/wiki/Bad_Brains\n",
            "Summary: Bad Brains are an American rock band formed in Washington, D.C. in 1976. Rolling Stone magazine called them \"the mother of all black hard-rock bands\",[6] and they have been cited as a seminal influence to numerous subgenres of heavy metal, including thrash/speed metal, alternative metal, funk metal and rap/nu metal.[7][8] Bad Brains are followers of the Rastafari movement.[5]\n",
            "Bad Brains have released nine studio albums. This lineup was intact until 1987 and has reunited periodically in the years since. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I ackowledge I've used AI tools (OpenAI ChatGPT) to help me for the summarised_text functions. I understand the syntax and basic concepts of whats going on with these functions; filtering out stop words (a, the, and), finding the word frequency to find the most common words and combine the most common words together.\n",
        "\n",
        "At times, the script does not work or comes out with output as 'None'. I believe this is because some webpages cannot be scraped. Here are a few which work:\n",
        "\n",
        "*   https://en.wikipedia.org/wiki/Bad_Brains\n",
        "*   https://en.wikipedia.org/wiki/Minor_Threat\n",
        "\n"
      ],
      "metadata": {
        "id": "gdO4lyQeXcPG"
      }
    }
  ]
}