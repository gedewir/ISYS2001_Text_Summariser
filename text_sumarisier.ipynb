{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_sumarisier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gedewir/ISYS2001_Text_Summariser/blob/main/text_sumarisier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarisation\n",
        "\n",
        "Recall an earlier notebook where you were given a task to summarise online content and produce a report. We converted an audio file to text. Similarly, we could write a notebook to convert a PDF or Word document to text or Web pages to text. The strategy is to convert everything to text, summarise the text, and use the summary in the final report.\n",
        "\n",
        "# The Challenge\n",
        "\n",
        "Create a project to summarise text and publish the project in Binder. \n",
        "\n",
        "You can choose how to input the text.  Some ideas include pasting it into a string, reading from a file, extract from a PDF or a webpage. \n",
        " \n",
        "It is okay to follow a online tutoirial or youtube video but make sure you have some understanding of what you are doing. You can ask you tutor for help if needed.  They will either help search, or perhaps explain the code in a tutorial.\n",
        "\n",
        "\n",
        "# Task 0 - Initialise a NEW repository\n",
        "\n",
        "We are going to deploy this notebook using Binder.\n",
        "\n",
        "* Initialise a new PUBLIC GitHub repository, say called, text_summariser.\n",
        "* Import this notebook into the new repository\n",
        "\n"
      ],
      "metadata": {
        "id": "SmVEXRTA1wZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ## Task 1 - State the Problem Clearly\n",
        "\n",
        "* The problem is that we need to summarise website page so it can be read within a few sentences rather than the whole thing itself, like \"Bad Brains is a punk rock band formed in Washington, D.C,...\"\n",
        "# Task 2 - Describe Inputs & Outputs\n",
        "* We want a URL input,e.g https://en.wikipedia.org/wiki/Bad_Brains, and output after the script is finished by print()\n",
        "# Task 3 - Work the problem by hand\n",
        "* We're using PyInputPlus for input validation, urllib for web scraping and  Spacy for the NLP module and text summarisation therefore we need to install that from pip and import them firstly\n",
        "* The user will be asked for a URL and if it is not a valid URL\n",
        "* If the URL is valid, URLLib will then make a request and the text will be a variable\n",
        "* We need to state that we're using the English tokenizer in Spacy as the variable 'nlp'\n",
        "* We'll pass the text variable to the nlp\n",
        "* Output the text using print()\n",
        "# Task 4 Develop an algorithm\n"
      ],
      "metadata": {
        "id": "AqwrQQU0PpwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install modules\n",
        "\n",
        "#ask the user for URL\n",
        "\n",
        "# function to make a URL request\n",
        "  # GET HTTP request\n",
        "  # retrieve & return only the text\n",
        "\n",
        "# use English tokenizer\n",
        "\n",
        "# function to process text, extract sentences & join all together\n",
        "  # feed text to nlp\n",
        "  # extract sentences\n",
        "  # join & return sentences together "
      ],
      "metadata": {
        "id": "SnF-cctxoYgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing all modules:\n",
        "!pip install -q requests\n",
        "!pip install -q spacy\n",
        "!pip install -q PyInputPlus"
      ],
      "metadata": {
        "id": "gFWMCGhRT76m",
        "outputId": "6ad1e2e6-793a-4442-fb14-41c559735e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyInputPlus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pysimplevalidate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stdiomask (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all modules\n",
        "import requests \n",
        "import spacy\n",
        "import pyinputplus as pyip\n",
        "\n",
        "# ask the user to input a URL\n",
        "user_url = pyip.inputURL('Enter a URL: ')\n",
        "\n",
        "# function to make url request and only output the body text\n",
        "def url_request(requested_url):\n",
        "  url_response = requests.get(requested_url)\n",
        "  url_body_text = url_response.text\n",
        "  return url_body_text\n",
        "\n",
        "user_body_text = url_request(user_url)\n",
        "\n",
        "# state that we are using the English tokenzier\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function to process the text, extract the sentences and join it altogether using spacy nlp\n",
        "def summarised_text(text):\n",
        "  doc = nlp(text)\n",
        "  sentences = [sent.text for sent in doc.sents]\n",
        "  summary = \" \".join(sentences[:3]) + \"...\"\n",
        "  return summary\n",
        "\n",
        "user_url_summarised = summarised_text(user_body_text)\n",
        "\n",
        "print(user_url_summarised)"
      ],
      "metadata": {
        "id": "jJYeHDMcVtOR",
        "outputId": "d5668dd5-43e0-41ce-f865-0354098478b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a URL: https://en.wikipedia.org/wiki/Her_(film)\n",
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\" lang=\"en\" dir=\"ltr\">\n",
            "<head>\n",
            "<meta charset=\"UTF-8\"/>\n",
            "<title>Her - Wikipedia</title>\n",
            "<script>document.documentElement.className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\";(function(){var cookie=document.cookie.match(/(?:^|; )enwikimwclientprefs=([^;]+)/);if(cookie){var featureName=cookie[1];document.documentElement.className=document.documentElement.className.replace(featureName+'-enabled',featureName+'-disabled');}}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\n",
            "\"51dffef0-0e94-4498-97d5-eb44843d9fa6\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Her\",\"wgTitle\":\"Her\",\"wgCurRevisionId\":1137226881,\"wgRevisionId\":1137226881,\"wgArticleId\":839716,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Short description is different from Wikidata\",\"All article disambiguation pages\",\"All disambiguation pages\",\"Disambiguation pages\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Her\",\"wgRelevantArticleId\":839716,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\n",
            "\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":2000,\"wgNoticeProject\":\"wikipedia\",\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":10,\"wgULSCurrentAutonym\":\"English\",\"wgEditSubmitButtonLabelPublish\":true,\"wgCentralAuthMobileDomain\":false,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q1428920\",\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"skins.vector.user.styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"skins.vector.user\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"mediawiki.ui.button\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"mediawiki.ui.icon\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\n",
            "\"ext.wikimediaBadges\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"site\",\"mediawiki.page.ready\",\"mediawiki.toc\",\"skins.vector.js\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget. ReferenceTooltips\",\"ext.gadget.charinsert\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.echo.centralauth\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.cx.uls.quick.actions\",\"wikibase.client.vector-2022\",\"ext.growthExperiments. SuggestedEditSession\"];</script>\n",
            "<script>(RLQ=window....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script outputs HTML and is not suffcient to produce an output as a summary.\n",
        "\n",
        "Based off research, the BeatifulSoup module allows us to parse HTML.\n",
        "\n",
        "I think we need to parse the HTML text first and then also summarise the whole entire webpage.\n",
        "\n"
      ],
      "metadata": {
        "id": "c7m2urigdZoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install modules\n",
        "\n",
        "#ask the user for URL\n",
        "\n",
        "# function to make a URL request\n",
        "  # GET HTTP request\n",
        "  # retrieve & return only the text\n",
        "\n",
        "# function to parse HTML to plaintext\n",
        "  # parse HTML\n",
        "  # find text that is within the <p> class\n",
        "  # join the <p> text together\n",
        "\n",
        "# state the fact we are using English tokenzier\n",
        "\n",
        "# function to summarise text\n",
        "  # feed text to nlp\n",
        "  # filter out stop words\n",
        "  # add repeating words together\n",
        "  # rank words off word frequency\n",
        "  # combine and return top 3 sentences\n",
        "\n",
        "# use all functions from start to finish"
      ],
      "metadata": {
        "id": "hDch4qZApNSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install beatiful soup 4\n",
        "!pip install -q bs4"
      ],
      "metadata": {
        "id": "u20A_87nea0F",
        "outputId": "05c84406-d078-41b7-b072-2d813f244661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all modules\n",
        "import bs4 as bs\n",
        "import requests \n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "import pyinputplus as pyip\n",
        "\n",
        "# ask the user to input a URL\n",
        "user_url = pyip.inputURL('Enter a URL: ')\n",
        "\n",
        "# function to make url request and only output the body text\n",
        "def url_request(requested_url):\n",
        "  url_response = requests.get(requested_url)\n",
        "  url_body_text = url_response.text\n",
        "  return url_body_text\n",
        "\n",
        "# use bs4 to parse html of the requested URL\n",
        "def soupify_html(html_text):\n",
        "  soup = bs.BeautifulSoup(html_text, 'html.parser')\n",
        "  body_text = soup.find_all('p')\n",
        "  text = ''\n",
        "  for p in body_text:\n",
        "    text = text + p.get_text()\n",
        "  return text\n",
        "\n",
        "# state that we are using the English tokenzier\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function to process the text, extract the sentences and join it altogether using spacy nlp\n",
        "def summarised_text(text):\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # filter out stop words in text\n",
        "  stopwords = list(STOP_WORDS)\n",
        "  words = [token.text for token in doc if token.text not in stopwords and token.text not in punctuation]\n",
        "\n",
        "  # add repeating words together\n",
        "  word_frequency = {}\n",
        "  for word in text:\n",
        "    if word not in word_frequency:\n",
        "      word_frequency[word] = 1\n",
        "    else:\n",
        "      word_frequency[word] += 1 \n",
        "  # rank words on word frequency\n",
        "    sentence_scores = {}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequency.keys():\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequency[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequency[word.text.lower()]\n",
        "\n",
        "    # get the top 3 sentences with the highest scores\n",
        "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
        "\n",
        "    # concatenate the top sentences to generate the summary\n",
        "    summary = \"\"\n",
        "    for sentence in top_sentences:\n",
        "        summary += sentence.text.strip() + \" \"\n",
        "\n",
        "    return summary\n",
        "\n",
        "user_body_text = url_request(user_url)\n",
        "user_soup_text = soupify_html(user_body_text)\n",
        "user_url_summarised = summarised_text(user_soup_text)\n",
        "print('Summary:', user_url_summarised)"
      ],
      "metadata": {
        "id": "s9iCuO8Hekgp",
        "outputId": "f5e261ad-f4fe-4860-fa28-8c19e1956382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a URL: https://en.wikipedia.org/wiki/Bad_Brains\n",
            "Summary: Bad Brains are an American rock band formed in Washington, D.C. in 1976. Rolling Stone magazine called them \"the mother of all black hard-rock bands\",[6] and they have been cited as a seminal influence to numerous subgenres of heavy metal, including thrash/speed metal, alternative metal, funk metal and rap/nu metal.[7][8] Bad Brains are followers of the Rastafari movement.[5]\n",
            "Bad Brains have released nine studio albums. This lineup was intact until 1987 and has reunited periodically in the years since. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I ackowledge I've used AI tools (OpenAI ChatGPT) to help me for the summarised_text functions. I understand the syntax and basic concepts of whats going on with these functions; filtering out stop words (a, the, and), finding the word frequency to find the most common words and combine the most common words together.\n",
        "\n",
        "At times, the script does not work or comes out with output as 'None'. I believe this is because some webpages cannot be scraped. Here are a few which work:\n",
        "\n",
        "*   https://en.wikipedia.org/wiki/Bad_Brains\n",
        "*   https://en.wikipedia.org/wiki/Minor_Threat\n",
        "\n"
      ],
      "metadata": {
        "id": "gdO4lyQeXcPG"
      }
    }
  ]
}