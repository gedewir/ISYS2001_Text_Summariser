{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_sumarisier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gedewir/ISYS2001_Text_Summariser/blob/main/text_sumarisier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarisation\n",
        "\n",
        "Recall an earlier notebook where you were given a task to summarise online content and produce a report. We converted an audio file to text. Similarly, we could write a notebook to convert a PDF or Word document to text or Web pages to text. The strategy is to convert everything to text, summarise the text, and use the summary in the final report.\n",
        "\n",
        "# The Challenge\n",
        "\n",
        "Create a project to summarise text and publish the project in Binder. \n",
        "\n",
        "You can choose how to input the text.  Some ideas include pasting it into a string, reading from a file, extract from a PDF or a webpage. \n",
        " \n",
        "It is okay to follow a online tutoirial or youtube video but make sure you have some understanding of what you are doing. You can ask you tutor for help if needed.  They will either help search, or perhaps explain the code in a tutorial.\n",
        "\n",
        "\n",
        "# Task 0 - Initialise a NEW repository\n",
        "\n",
        "We are going to deploy this notebook using Binder.\n",
        "\n",
        "* Initialise a new PUBLIC GitHub repository, say called, text_summariser.\n",
        "* Import this notebook into the new repository\n",
        "\n"
      ],
      "metadata": {
        "id": "SmVEXRTA1wZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ## Task 1 - State the Problem Clearly\n",
        "\n",
        "* The problem is that we need to summarise website page so it can be read within a few sentences rather than the whole thing itself, like \"Bad Brains is a punk rock band formed in Washington, D.C,...\"\n",
        "# Task 2 - Describe Inputs & Outputs\n",
        "* We want a URL input,e.g https://en.wikipedia.org/wiki/Bad_Brains, and output after the script is finished by print()\n",
        "# Task 3 - Work the problem by hand\n",
        "* We're using PyInputPlus for input validation, urllib for web scraping and  Spacy for the NLP module and text summarisation therefore we need to install that from pip and import them firstly\n",
        "* The user will be asked for a URL and if it is not a valid URL\n",
        "* If the URL is valid, URLLib will then make a request and the text will be a variable\n",
        "* We need to state that we're using the English tokenizer in Spacy as the variable 'nlp'\n",
        "* We'll pass the text variable to the nlp\n",
        "* Output the text using print()\n"
      ],
      "metadata": {
        "id": "AqwrQQU0PpwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing all modules:\n",
        "!pip install -q requests\n",
        "!pip install -q spacy\n",
        "!pip install -q PyInputPlus"
      ],
      "metadata": {
        "id": "gFWMCGhRT76m",
        "outputId": "101dc72d-5b6d-45f2-b26b-b2a7f92e7212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all modules\n",
        "import requests \n",
        "import spacy\n",
        "import pyinputplus as pyip\n",
        "\n",
        "# ask the user to input a URL\n",
        "user_url = pyip.inputURL('Enter a URL: ')\n",
        "\n",
        "# function to make url request and only output the body text\n",
        "def url_request(requested_url):\n",
        "  url_response = requests.get(requested_url)\n",
        "  url_body_text = url_response.text\n",
        "  return url_body_text\n",
        "\n",
        "user_body_text = url_request(user_url)\n",
        "\n",
        "# state that we are using the English tokenzier\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function to process the text, extract the sentences and join it altogether using spacy nlp\n",
        "def summarised_text(text):\n",
        "  doc = nlp(text)\n",
        "  sentences = [sent.text for sent in doc.sents]\n",
        "  summary = \" \".join(sentences[:3]) + \"...\"\n",
        "  return summary\n",
        "\n",
        "user_url_summarised = summarised_text(user_body_text)\n",
        "\n",
        "print(user_url_summarised)"
      ],
      "metadata": {
        "id": "jJYeHDMcVtOR",
        "outputId": "d5668dd5-43e0-41ce-f865-0354098478b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a URL: https://en.wikipedia.org/wiki/Her_(film)\n",
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\" lang=\"en\" dir=\"ltr\">\n",
            "<head>\n",
            "<meta charset=\"UTF-8\"/>\n",
            "<title>Her - Wikipedia</title>\n",
            "<script>document.documentElement.className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled\";(function(){var cookie=document.cookie.match(/(?:^|; )enwikimwclientprefs=([^;]+)/);if(cookie){var featureName=cookie[1];document.documentElement.className=document.documentElement.className.replace(featureName+'-enabled',featureName+'-disabled');}}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\n",
            "\"51dffef0-0e94-4498-97d5-eb44843d9fa6\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Her\",\"wgTitle\":\"Her\",\"wgCurRevisionId\":1137226881,\"wgRevisionId\":1137226881,\"wgArticleId\":839716,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Short description is different from Wikidata\",\"All article disambiguation pages\",\"All disambiguation pages\",\"Disambiguation pages\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Her\",\"wgRelevantArticleId\":839716,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\n",
            "\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":2000,\"wgNoticeProject\":\"wikipedia\",\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":10,\"wgULSCurrentAutonym\":\"English\",\"wgEditSubmitButtonLabelPublish\":true,\"wgCentralAuthMobileDomain\":false,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q1428920\",\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"skins.vector.user.styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"skins.vector.user\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"mediawiki.ui.button\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"mediawiki.ui.icon\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\n",
            "\"ext.wikimediaBadges\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"site\",\"mediawiki.page.ready\",\"mediawiki.toc\",\"skins.vector.js\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.cx.eventlogging.campaigns\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget. ReferenceTooltips\",\"ext.gadget.charinsert\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.echo.centralauth\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.cx.uls.quick.actions\",\"wikibase.client.vector-2022\",\"ext.growthExperiments. SuggestedEditSession\"];</script>\n",
            "<script>(RLQ=window....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script outputs HTML and is not suffcient to produce an output as a summary.\n",
        "\n",
        "Based off research, the BeatifulSoup module allows us to parse HTML.\n",
        "\n",
        "I think we need to parse the HTML text first and then feed it to the NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "c7m2urigdZoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install beatiful soup 4\n",
        "!pip install -q bs4"
      ],
      "metadata": {
        "id": "u20A_87nea0F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all modules\n",
        "import bs4 as bs\n",
        "import requests \n",
        "import spacy\n",
        "import pyinputplus as pyip\n",
        "\n",
        "# ask the user to input a URL\n",
        "user_url = pyip.inputURL('Enter a URL: ')\n",
        "\n",
        "# function to make url request and only output the body text\n",
        "def url_request(requested_url):\n",
        "  url_response = requests.get(requested_url)\n",
        "  url_body_text = url_response.text\n",
        "  return url_body_text\n",
        "\n",
        "user_body_text = url_request(user_url)\n",
        "\n",
        "# use bs4 to parse html of the requested URL\n",
        "def soupify_html(html_text):\n",
        "  soup = bs.BeautifulSoup(html_text, 'html.parser')\n",
        "  find_body = soup.find('div', {'class': 'mw-parser-output'})\n",
        "  body_text = find_body.find_all('p')\n",
        "  text = ''\n",
        "  for p in body_text:\n",
        "    text = text + str(p)\n",
        "  return text\n",
        "\n",
        "user_soup_text = soupify_html(user_body_text)\n",
        "\n",
        "print(user_soup_text)\n",
        "# state that we are using the English tokenzier\n",
        "#nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function to process the text, extract the sentences and join it altogether using spacy nlp\n",
        "#def summarised_text(text):\n",
        "#  doc = nlp(text)\n",
        "#  summary = doc._.textrank.summary()\n",
        "#  return summary\n",
        "\n",
        "#user_url_summarised = summarised_text(user_soup_text)\n",
        "\n",
        "#print(user_url_summarised)"
      ],
      "metadata": {
        "id": "s9iCuO8Hekgp",
        "outputId": "e395671d-4025-4093-b77e-e4d4a80fd3db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a URL: "
          ]
        }
      ]
    }
  ]
}